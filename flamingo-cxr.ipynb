{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:31:28.178781Z",
     "start_time": "2025-05-13T23:42:46.571033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import os\n",
    "import pandas as pd\n",
    "import inspect\n",
    "if not hasattr(inspect, 'formatargspec'):\n",
    "    inspect.formatargspec = inspect.formatargvalues\n",
    "import random\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, ViTModel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MultilabelF1Score, MultilabelAUROC\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "# Configure environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "LABEL_COLS = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n",
    "    'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "    'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n",
    "]\n",
    "\n",
    "\n",
    "RARE_LABELS = ['Fracture', 'Pleural Other', 'Enlarged Cardiomediastinum', 'Pneumothorax']\n",
    "\n",
    "\n",
    "# Custom loss function that balances class distribution and focuses on hard samples for multilabel classification\n",
    "class DistributionBalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, class_freq, beta=0.9999, gamma=2, eps=1e-6):\n",
    "        super().__init__()     # Compute class weights based on effective number of samples\n",
    "        self.gamma = gamma\n",
    "        effective_num = 1.0 - torch.pow(torch.tensor(beta), class_freq)\n",
    "        self.weights = (1.0 - beta) / (effective_num + eps)\n",
    "        self.weights = self.weights / self.weights.sum() * len(class_freq)\n",
    "\n",
    "    def forward(self, inputs, targets):  # Computes the focal loss with class balancing for multilabel targets\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.weights.to(inputs.device) * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Sampler to ensure each batch contains a balanced number of rare and common class samples\n",
    "class BalancedMultilabelSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, rare_labels, samples_per_class=4, batch_size=16):\n",
    "         # Initializes sampler with dataset and rare label balancing parameters\n",
    "        self.dataset = dataset\n",
    "        self.rare_labels = rare_labels\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Calculate rare indices\n",
    "        self.rare_indices = []\n",
    "        for idx, row in enumerate(dataset.df[rare_labels].values):\n",
    "            if any(row == 1):\n",
    "                self.rare_indices.append(idx)\n",
    "\n",
    "        self.common_indices = [i for i in range(len(dataset))\n",
    "                              if i not in self.rare_indices]\n",
    "\n",
    "        # Fix n_batches calculation\n",
    "        self.n_batches = (len(self.rare_indices) // samples_per_class) + \\\n",
    "                        (len(self.common_indices) // (batch_size - samples_per_class))\n",
    "\n",
    "    def __iter__(self):           # Yields indices to form balanced batches of rare and common samples\n",
    "        for _ in range(self.n_batches):\n",
    "            # Sample with replacement if needed\n",
    "            rare = random.choices(self.rare_indices, k=self.samples_per_class)\n",
    "            common = random.sample(self.common_indices, k=self.batch_size - self.samples_per_class)\n",
    "            yield from rare + common  # Individual indices\n",
    "\n",
    "    def __len__(self):           # Returns total number of samples in an epoch\n",
    "        return self.n_batches * self.batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Utility function to print the distribution of positive samples for each label in a dataset split\n",
    "def print_label_distribution(df, split_name):\n",
    "    print(f\"\\nLabel distribution in {split_name} set:\")\n",
    "    label_counts = df[LABEL_COLS].apply(lambda x: (x == 1).sum())\n",
    "    total_samples = len(df)\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} positive samples ({count/total_samples:.2%})\")\n",
    "    print(f\"Total samples in {split_name}: {total_samples}\\n\")\n",
    "\n",
    "NUM_CLASSES = len(LABEL_COLS)\n",
    "BATCH_SIZE = 16\n",
    "ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 10\n",
    "THRESHOLD = 0.5\n",
    "MAX_LENGTH = 256\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom focal loss for multilabel classification with optional masking for uncertain labels\n",
    "class MultiLabelFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, pos_weight=None):           # Initializes focal loss parameters for multilabel tasks\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):         # Computes the masked focal loss for multilabel classification\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none', pos_weight=self.pos_weight)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        mask = (targets != 0.5).float()  # Mask uncertain labels\n",
    "        return (focal_loss * mask).mean()\n",
    "\n",
    "def get_stratified_subset(df, n_samples):          # Returns a stratified subset of the dataframe, preserving multilabel distribution\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=len(df)-n_samples, random_state=42)\n",
    "    for train_idx, _ in msss.split(df, df[LABEL_COLS]):\n",
    "        return df.iloc[train_idx]\n",
    "\n",
    "def get_cached_splits():        # Loads or generates and caches train/val/test splits, ensuring stratification and valid file paths\n",
    "    cache_files = [\"train_df.parquet\", \"val_df.parquet\", \"test_df.parquet\"]\n",
    "    if all(os.path.exists(f) for f in cache_files):\n",
    "        logger.info(\"Loading cached splits\")\n",
    "        return (\n",
    "            pd.read_parquet(\"train_df.parquet\"),\n",
    "            pd.read_parquet(\"val_df.parquet\"),\n",
    "            pd.read_parquet(\"test_df.parquet\")\n",
    "        )\n",
    "\n",
    "    logger.info(\"Generating new splits\")\n",
    "    df1 = pd.read_csv('cxr-record-list.csv.gz')\n",
    "    df2 = pd.read_csv('mimic-cxr-2.0.0-chexpert.csv')\n",
    "    df3 = pd.read_csv('mimic-cxr-2.0.0-metadata.csv')\n",
    "\n",
    "    merge_df = df1.merge(df2, on=['subject_id', 'study_id'], how='inner')\n",
    "    merge_df = merge_df.merge(df3[['dicom_id', 'ViewPosition']], on='dicom_id', how='inner')\n",
    "\n",
    "    with open('IMAGE_FILENAMES.txt', 'r') as f:\n",
    "        image_paths = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    image_paths = [p for p in image_paths if p.startswith(('files/p10/', 'files/p11/', 'files/p12/'))]\n",
    "    dicom_to_path = {os.path.splitext(os.path.basename(p))[0]: p for p in image_paths}\n",
    "\n",
    "    df = merge_df.copy()\n",
    "    df['image_rel_path'] = df['dicom_id'].astype(str).map(dicom_to_path)\n",
    "    df = df[df['image_rel_path'].notnull()]\n",
    "\n",
    "    # Enhanced label processing\n",
    "    df[LABEL_COLS] = df[LABEL_COLS].replace(-1, 0.5).fillna(0.0)\n",
    "\n",
    "    df['image_path'] = df['image_rel_path'].apply(lambda x: os.path.join('mimic-cxr-jpg', '2.1.0', x))\n",
    "\n",
    "    def build_report_path(row):         # Generates the file path for each radiology report using subject and study IDs from the DataFrame row\n",
    "        p_prefix = f\"p{str(row['subject_id'])[:2]}\"\n",
    "        return os.path.join(\n",
    "            'mimic-cxr-reports', 'files', p_prefix,\n",
    "            f\"p{row['subject_id']}\", f\"s{row['study_id']}.txt\"\n",
    "        )\n",
    "\n",
    "    df['report_path'] = df.apply(build_report_path, axis=1)\n",
    "    df = df[df['image_path'].apply(os.path.exists) & df['report_path'].apply(os.path.exists)]\n",
    "\n",
    "    def get_stratified_splits(df):              # Splits the dataset into train/val/test (60/20/20) using multilabel stratification to preserve label distributions, then caches splits as Parquet files\n",
    "        y = df[LABEL_COLS].values\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        train_idx, temp_idx = next(msss.split(df, y))\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "        val_idx, test_idx = next(msss_val.split(df.iloc[temp_idx], df.iloc[temp_idx][LABEL_COLS].values))\n",
    "        return df.iloc[train_idx], df.iloc[temp_idx].iloc[val_idx], df.iloc[temp_idx].iloc[test_idx]\n",
    "\n",
    "    train_df, val_df, test_df = get_stratified_splits(df)\n",
    "    train_df.to_parquet(\"train_df.parquet\")\n",
    "    val_df.to_parquet(\"val_df.parquet\")\n",
    "    test_df.to_parquet(\"test_df.parquet\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "class MemoryOptimizedDataset(Dataset):              # Memory-efficient dataset for loading images, reports, and labels with augmentations and error handling\n",
    "    def __init__(self, df, tokenizer, rare_labels):   # Initializes dataset with data, tokenizer, rare label info, and sets up augmentations\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rare_labels = rare_labels\n",
    "        self.rare_indices = []\n",
    "        self.common_indices = []\n",
    "\n",
    "        # Precompute rare indices\n",
    "        for idx, row in df.iterrows():\n",
    "            if any(row[label] == 1 for label in self.rare_labels):\n",
    "                self.rare_indices.append(idx)\n",
    "\n",
    "        # Base augmentations\n",
    "        self.base_transform = A.Compose([\n",
    "            A.Resize(height=256, width=256),  # ✅ Explicit parameter names\n",
    "            A.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # ✅ Tuple for size\n",
    "            A.Rotate(limit=10),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        # Strong augmentations for rare classes\n",
    "        self.rare_transform = A.Compose([\n",
    "            A.RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0)),  # Fixed here\n",
    "            A.Resize(height=256, width=256),\n",
    "            A.RandomResizedCrop(size=(224, 224), scale=(0.4, 1.0)),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(limit=45, p=0.7),\n",
    "            A.ElasticTransform(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.CLAHE(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        self.valid_indices = self._validate_files()\n",
    "        self.sample_weights = self._calculate_sample_weights()\n",
    "\n",
    "    def _calculate_sample_weights(self):        # Calculates sample weights based on inverse class frequency\n",
    "        # Calculate inverse class frequencies\n",
    "        class_counts = self.df[LABEL_COLS].sum(axis=0)\n",
    "        class_weights = 1 / (class_counts + 1e-6)\n",
    "\n",
    "        # Assign sample weight = sum of weights for its positive labels\n",
    "        sample_weights = self.df[LABEL_COLS].apply(\n",
    "            lambda x: sum(class_weights[x == 1]),\n",
    "            axis=1\n",
    "        )\n",
    "        return sample_weights.values\n",
    "\n",
    "    def _validate_files(self):          # Validates image and report file existence, logging any missing files\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.df)):\n",
    "            row = self.df.iloc[idx]\n",
    "            if os.path.exists(row['image_path']) and os.path.exists(row['report_path']):\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                logger.warning(f\"Missing files for index {idx}\")\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):               # Returns number of valid samples in the dataset\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):             # Loads and returns a transformed image, multilabels, and tokenized report for a given index and retries up to 5 times if loading fails, returns dummy data if all attempts fail\n",
    "        for attempt in range(5):  # Try up to 5 times\n",
    "            try:\n",
    "                idx = int(idx)\n",
    "                actual_idx = idx\n",
    "                row = self.df.iloc[actual_idx]\n",
    "\n",
    "                # Load image\n",
    "                image = Image.open(row['image_path']).convert(\"RGB\")\n",
    "                image_np = np.array(image)\n",
    "\n",
    "                # Apply rare or base transforms\n",
    "                if actual_idx in self.rare_indices:\n",
    "                    transformed = self.rare_transform(image=image_np)\n",
    "                else:\n",
    "                    transformed = self.base_transform(image=image_np)\n",
    "                image_tensor = transformed[\"image\"]\n",
    "\n",
    "                # Load and tokenize report\n",
    "                with open(row['report_path'], 'r', encoding='utf-8') as f:\n",
    "                    report = f.read().strip()\n",
    "\n",
    "                tokens = self.tokenizer(\n",
    "                    report[:500],\n",
    "                    max_length=256,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                return (\n",
    "                    image_tensor,\n",
    "                    torch.FloatTensor(row[LABEL_COLS].values.astype(np.float32)),\n",
    "                    tokens.input_ids.squeeze(0).long(),\n",
    "                    tokens.attention_mask.squeeze(0)\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
    "                # Use rare_indices and common_indices for retry\n",
    "                if self.rare_indices and self.common_indices:\n",
    "                    idx = random.choice(self.rare_indices + self.common_indices)\n",
    "                else:\n",
    "                    idx = random.randint(0, len(self.df) - 1)\n",
    "\n",
    "        # Fallback to dummy data after 5 failed attempts\n",
    "        logger.error(f\"Failed to load sample after 5 attempts. Returning dummy data.\")\n",
    "        return (\n",
    "            torch.randn(3, 224, 224),\n",
    "            torch.zeros(len(LABEL_COLS)),\n",
    "            torch.zeros(256, dtype=torch.long),\n",
    "            torch.zeros(256)\n",
    "        )\n",
    "\n",
    "\n",
    "class EfficientFlamingo(nn.Module):   # Multimodal model combining vision (ViT) and text (BioClinicalBERT) encoders for classification and report generation\n",
    "    def __init__(self, num_classes=NUM_CLASSES, vocab_size=None):       # Initializes vision and text encoders, fusion layers, classifier, and report head\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.text_encoder = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "        # Freeze first 6 layers of text encoder\n",
    "        text_encoder_params = list(self.text_encoder.parameters())\n",
    "        for param in text_encoder_params[:6]:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.vision_proj = nn.Linear(768, 256)\n",
    "        self.text_proj = nn.Linear(768, 256)\n",
    "\n",
    "        # Modified fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Changed report head to output sequence\n",
    "        self.report_head = nn.Linear(256, vocab_size)\n",
    "\n",
    "    def forward(self, images, input_ids=None, attention_mask=None):          # Computes fused vision and text features for multilabel classification and report generation\n",
    "        # Image features\n",
    "        vision_features = self.vision_encoder(images).last_hidden_state.mean(1)\n",
    "        vision_features = self.vision_proj(vision_features)\n",
    "\n",
    "        # Text features (keep sequence dimension)\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state  # [batch, seq_len, 768]\n",
    "        text_features = self.text_proj(text_features)   # [batch, seq_len, 256]\n",
    "\n",
    "        # Fuse features for each token in sequence\n",
    "        fused = torch.cat([\n",
    "            vision_features.unsqueeze(1).expand(-1, text_features.size(1), -1),\n",
    "            text_features\n",
    "        ], dim=-1)  # [batch, seq_len, 512]\n",
    "\n",
    "        fused = self.fusion(fused)  # [batch, seq_len, 256]\n",
    "\n",
    "        # Classification (global average pooling)\n",
    "        cls_logits = self.classifier(fused.mean(1))\n",
    "\n",
    "        # Report generation (per-token predictions)\n",
    "        report_logits = self.report_head(text_features)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "        return cls_logits, report_logits\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size, sampler=None):       # Creates a DataLoader for a given dataset and optional sampler\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scaler, device):       # Trains the model for one epoch, computes combined classification and report generation loss, and updates weights\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(total=len(loader), desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (images, labels, input_ids, attn_mask) in enumerate(loader):\n",
    "        try:\n",
    "            # Move data to device FIRST\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            input_ids = input_ids.to(device, non_blocking=True)\n",
    "            attn_mask = attn_mask.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision context\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                # Forward pass\n",
    "                cls_logits, report_logits = model(images, input_ids, attn_mask)\n",
    "\n",
    "                # Classification loss\n",
    "                loss_cls = criterion[0](cls_logits, labels)\n",
    "\n",
    "                # Report generation loss (FIXED DIMENSIONS)\n",
    "                loss_report = criterion[1](\n",
    "                    report_logits.view(-1, report_logits.size(-1)),  # [batch_size*seq_len, vocab_size]\n",
    "                    input_ids.view(-1)                               # [batch_size*seq_len]\n",
    "                )\n",
    "\n",
    "                # Combined loss\n",
    "                loss = 0.7 * loss_cls + 0.3 * loss_report\n",
    "\n",
    "            # Backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"CLS Loss\": f\"{loss_cls.item():.4f}\",\n",
    "                \"RPT Loss\": f\"{loss_report.item():.4f}\"\n",
    "            })\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {batch_idx}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    progress_bar.close()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device, display_samples=0):     # Evaluates the model on validation/test data, computes classification and report generation metrics\n",
    "    model.eval()\n",
    "    f1_metric.reset()\n",
    "    auroc_metric.reset()\n",
    "    per_class_f1_metric = MultilabelF1Score(num_labels=NUM_CLASSES, average=None, threshold=THRESHOLD).to(device)\n",
    "\n",
    "    # Initialize text metrics\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    import rougescore\n",
    "\n",
    "\n",
    "    from nltk.translate.bleu_score import SmoothingFunction\n",
    "    from nltk import word_tokenize\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=UserWarning, module='nltk.translate.bleu_score')\n",
    "    smooth = SmoothingFunction().method4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, input_ids, attn_mask in loader:\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            cls_logits, report_logits = model(images, input_ids, attn_mask)\n",
    "            probs = torch.sigmoid(cls_logits)\n",
    "\n",
    "            # Update classification metrics\n",
    "            f1_metric.update(probs, labels.int())\n",
    "            auroc_metric.update(probs, labels.int())\n",
    "            per_class_f1_metric.update(probs, labels.int())\n",
    "\n",
    "            # Decode reports\n",
    "            pred_ids = report_logits.argmax(-1)\n",
    "            hyps = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                    for ids in pred_ids]\n",
    "            refs = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                    for ids in input_ids]\n",
    "\n",
    "            hypotheses.extend(hyps)\n",
    "            references.extend(refs)\n",
    "\n",
    "    # Compute classification metrics\n",
    "    macro_f1 = f1_metric.compute().item()\n",
    "    macro_auroc = auroc_metric.compute().item()\n",
    "    per_class_f1 = per_class_f1_metric.compute()\n",
    "\n",
    "\n",
    "    # Ensure hypotheses are strings\n",
    "    if isinstance(hypotheses[0], list):\n",
    "        hypotheses = [' '.join(h) for h in hypotheses]\n",
    "\n",
    "    # Ensure references is a list of lists of strings\n",
    "    if isinstance(references[0], str):\n",
    "        references = [[r] for r in references]\n",
    "\n",
    "    bleu_score = corpus_bleu(\n",
    "        hypotheses,\n",
    "        references,\n",
    "        tokenize='none'\n",
    "    ).score / 100\n",
    "\n",
    "\n",
    "    # Compute ROUGE-L using rougescore\n",
    "    rouge_scores = []\n",
    "    for hyp, ref in zip(hypotheses, references):\n",
    "        try:\n",
    "            score = rougescore.rouge_l(hyp, ref, alpha=0.5)\n",
    "            rouge_scores.append(score)\n",
    "        except:\n",
    "            rouge_scores.append(0.0)\n",
    "    avg_rouge = sum(rouge_scores)/len(rouge_scores) if rouge_scores else 0.0\n",
    "\n",
    "    # Log rare classes\n",
    "    logger.info(\"\\nRare Class Performance:\")\n",
    "    for cls in RARE_LABELS:\n",
    "        idx = LABEL_COLS.index(cls)\n",
    "        logger.info(f\"{cls}: {per_class_f1[idx]:.4f}\")\n",
    "\n",
    "    # Display first 5 samples if requested\n",
    "    if display_samples > 0:\n",
    "        logger.info(\"\\nGenerated Reports (First 5):\")\n",
    "        for i in range(min(5, len(hypotheses))):\n",
    "            logger.info(f\"\\nSample {i+1}:\")\n",
    "            logger.info(f\"Generated: {hypotheses[i]}\")\n",
    "            logger.info(f\"Reference: {references[i]}\")\n",
    "            logger.info(\"-\"*80)\n",
    "\n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'macro_auroc': macro_auroc,\n",
    "        'bleu4': bleu_score,\n",
    "        'rouge_l': avg_rouge\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_with_thresholds(model, loader, device, thresholds):   # Evaluates the model using class-specific thresholds for multilabel classification\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, input_ids, attention_mask in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            cls_logits, _ = model(images, input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(cls_logits)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Apply class-specific thresholds\n",
    "    preds = torch.zeros_like(all_probs)\n",
    "    for cls, thresh in thresholds.items():\n",
    "        if cls in LABEL_COLS:\n",
    "            idx = LABEL_COLS.index(cls)\n",
    "            preds[:, idx] = (all_probs[:, idx] > thresh).float()\n",
    "\n",
    "    # Compute metrics\n",
    "    macro_f1 = f1_score(all_labels.numpy(), preds.numpy(), average='macro', zero_division=0)\n",
    "    macro_auroc = safe_macro_roc_auc_score(all_labels.numpy(), all_probs.numpy())\n",
    "\n",
    "    return macro_f1, macro_auroc\n",
    "\n",
    "\n",
    "def safe_macro_roc_auc_score(y_true, y_score):          # Computes macro AUROC, skipping classes with only one label present\n",
    "\n",
    "    aucs = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_score_i = y_score[:, i]\n",
    "        if len(np.unique(y_true_i)) < 2:\n",
    "            continue  # skip this class\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true_i, y_score_i)\n",
    "            aucs.append(auc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if len(aucs) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(aucs)\n",
    "\n",
    "\n",
    "def identify_rare_labels(df, categorical_columns, threshold=0.05):          # Identifies rare labels in the dataset based on a frequency threshold\n",
    "    rare_labels = {}\n",
    "    for col in categorical_columns:\n",
    "        # Calculate positive class frequency (label=1)\n",
    "        pos_freq = df[col].mean()\n",
    "        # Store column name if positive samples are rare\n",
    "        if pos_freq < threshold:\n",
    "            rare_labels[col] = [1]  # Mark positive class as rare\n",
    "    return rare_labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":          # Main script: sets up device, data, model, training, and evaluation pipeline\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    assert torch.cuda.is_available(), \"CUDA not available!\"\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    logger.info(\"Loading data splits\")\n",
    "    train_df, val_df, test_df = get_cached_splits()\n",
    "\n",
    "    train_df = get_stratified_subset(train_df, 2000)\n",
    "    val_df = get_stratified_subset(val_df, 400)\n",
    "    test_df = get_stratified_subset(test_df, 400)\n",
    "\n",
    "    print_label_distribution(train_df, 'Training (used)')\n",
    "    print_label_distribution(val_df, 'Validation (used)')\n",
    "    print_label_distribution(test_df, 'Testing (used)')\n",
    "\n",
    "    thresholds = {\n",
    "        'Pleural Other': 0.15,\n",
    "        'Fracture': 0.10,\n",
    "        'Enlarged Cardiomediastinum': 0.20\n",
    "    }\n",
    "\n",
    "    # Calculate class frequencies from the ACTUAL training subset\n",
    "    class_freq = torch.tensor(\n",
    "        train_df[LABEL_COLS].replace(0.5, np.nan).mean(axis=0).values,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    categorical_columns = LABEL_COLS  # Use your existing label list\n",
    "    threshold = 0.05\n",
    "    rare_labels = identify_rare_labels(\n",
    "    train_df,\n",
    "    categorical_columns,  # Positional argument\n",
    "    threshold=threshold\n",
    ")\n",
    "\n",
    "    # Inverse frequency weighting (handle division by zero)\n",
    "    pos_weight = (1 - class_freq) / (class_freq + 1e-6)\n",
    "    pos_weight = pos_weight.to(device)\n",
    "\n",
    "    RARE_LABELS = ['Pleural Other', 'Fracture', 'Enlarged Cardiomediastinum']\n",
    "\n",
    "    logger.info(\"Initializing tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    logger.info(\"Creating datasets\")\n",
    "    train_dataset = MemoryOptimizedDataset(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        rare_labels=RARE_LABELS\n",
    "    )  # Add RARE_LABELS\n",
    "    val_dataset = MemoryOptimizedDataset(val_df, tokenizer, RARE_LABELS)  # Add RARE_LABELS\n",
    "\n",
    "    class_freq = torch.tensor(train_df[LABEL_COLS].mean(axis=0).values, dtype=torch.float32)\n",
    "\n",
    "    # Initialize balanced sampler and loss\n",
    "    sampler = BalancedMultilabelSampler(\n",
    "        train_dataset,\n",
    "        rare_labels=RARE_LABELS,\n",
    "        samples_per_class=4\n",
    "    )\n",
    "\n",
    "    criterion = (\n",
    "    DistributionBalancedFocalLoss(\n",
    "        class_freq=class_freq,\n",
    "        beta=0.99999,\n",
    "        gamma=5\n",
    "    ),\n",
    "    nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    ")\n",
    "\n",
    "    logger.info(\"Creating dataloaders\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    val_loader = create_dataloader(val_dataset, BATCH_SIZE)\n",
    "\n",
    "    logger.info(\"Initializing model\")\n",
    "    model = EfficientFlamingo(num_classes=NUM_CLASSES, vocab_size=len(tokenizer)).to(device)\n",
    "\n",
    "    f1_metric = MultilabelF1Score(num_labels=NUM_CLASSES, average='macro', threshold=0.5).to(device)\n",
    "    auroc_metric = MultilabelAUROC(num_labels=NUM_CLASSES, average='macro').to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.vision_encoder.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.text_encoder.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.fusion.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.report_head.parameters(), 'lr': 1e-4}\n",
    "    ], weight_decay=0.01)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-4,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    logger.info(\"Starting training\")\n",
    "    try:\n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            logger.info(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "\n",
    "            # Validation with metrics\n",
    "            val_metrics = validate(model, val_loader, device)\n",
    "\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"Loss: {train_loss:.4f} | \"\n",
    "                f\"Val F1: {val_metrics['macro_f1']:.4f} | \"\n",
    "                f\"BLEU-4: {val_metrics['bleu4']:.4f} | \"\n",
    "                f\"ROUGE-L: {val_metrics['rouge_l']:.4f}\"\n",
    "            )\n",
    "\n",
    "            if val_metrics['macro_f1'] > best_f1:\n",
    "                best_f1 = val_metrics['macro_f1']\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'thresholds': thresholds,\n",
    "                    'rare_labels': rare_labels,  # <-- ADD THIS LINE\n",
    "                }, 'best_model.pth')\n",
    "                logger.info(\"New best model saved\")\n",
    "\n",
    "            scheduler.step()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(f\"Training completed. Best validation Macro F1: {best_f1:.4f}\")\n",
    "\n",
    "    logger.info(\"\\nTesting with adjusted thresholds...\")\n",
    "\n",
    "    # Load checkpoint with device mapping\n",
    "    checkpoint = torch.load(\"best_model.pth\", map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Get preserved parameters from checkpoint\n",
    "    rare_labels = checkpoint.get('rare_labels', RARE_LABELS)\n",
    "    thresholds = checkpoint.get('thresholds', {\n",
    "        'Pleural Other': 0.3,\n",
    "        'Fracture': 0.25,\n",
    "        'Enlarged Cardiomediastinum': 0.35\n",
    "    })\n",
    "\n",
    "    # Reinitialize test dataset with proper parameters\n",
    "    test_dataset = MemoryOptimizedDataset(\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        rare_labels=rare_labels\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Run validation with sample display\n",
    "    test_metrics = validate(\n",
    "        model=model,\n",
    "        loader=test_loader,\n",
    "        device=device,\n",
    "        display_samples=5  # Show top 5 reports\n",
    "    )\n",
    "\n",
    "    # Log comprehensive results\n",
    "    logger.info(\"\\n=== Final Test Metrics ===\")\n",
    "    logger.info(f\"Clinical Metrics:\")\n",
    "    logger.info(f\"  Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
    "    logger.info(f\"  Macro AUROC: {test_metrics['macro_auroc']:.4f}\")\n",
    "    logger.info(f\"\\nReport Generation Metrics:\")\n",
    "    logger.info(f\"  BLEU-4: {test_metrics['bleu4']:.4f}\")\n",
    "    logger.info(f\"  ROUGE-L: {test_metrics['rouge_l']:.4f}\")\n",
    "\n",
    "    # Log threshold-adjusted performance if needed\n",
    "    if 'thresholds' in checkpoint:\n",
    "        logger.info(\"\\nThreshold-Adjusted Performance:\")\n",
    "        test_f1, test_auroc = validate_with_thresholds(\n",
    "            model,\n",
    "            test_loader,\n",
    "            device,\n",
    "            thresholds\n",
    "        )\n",
    "        logger.info(f\"  Macro F1: {test_f1:.4f}\")\n",
    "        logger.info(f\"  Macro AUROC: {test_auroc:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "9f88aa5b5ac18552",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\13joe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\13joe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Loading data splits\n",
      "INFO:__main__:Loading cached splits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA memory allocated: 0.00GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in Training (used) set:\n",
      "Atelectasis: 347 positive samples (17.35%)\n",
      "Cardiomegaly: 348 positive samples (17.40%)\n",
      "Consolidation: 77 positive samples (3.85%)\n",
      "Edema: 195 positive samples (9.75%)\n",
      "Enlarged Cardiomediastinum: 52 positive samples (2.60%)\n",
      "Fracture: 41 positive samples (2.05%)\n",
      "Lung Lesion: 56 positive samples (2.80%)\n",
      "Lung Opacity: 403 positive samples (20.15%)\n",
      "No Finding: 760 positive samples (38.00%)\n",
      "Pleural Effusion: 410 positive samples (20.50%)\n",
      "Pleural Other: 21 positive samples (1.05%)\n",
      "Pneumonia: 137 positive samples (6.85%)\n",
      "Pneumothorax: 76 positive samples (3.80%)\n",
      "Support Devices: 450 positive samples (22.50%)\n",
      "Total samples in Training (used): 2000\n",
      "\n",
      "\n",
      "Label distribution in Validation (used) set:\n",
      "Atelectasis: 69 positive samples (17.25%)\n",
      "Cardiomegaly: 70 positive samples (17.50%)\n",
      "Consolidation: 15 positive samples (3.75%)\n",
      "Edema: 39 positive samples (9.75%)\n",
      "Enlarged Cardiomediastinum: 10 positive samples (2.50%)\n",
      "Fracture: 8 positive samples (2.00%)\n",
      "Lung Lesion: 11 positive samples (2.75%)\n",
      "Lung Opacity: 81 positive samples (20.25%)\n",
      "No Finding: 152 positive samples (38.00%)\n",
      "Pleural Effusion: 82 positive samples (20.50%)\n",
      "Pleural Other: 4 positive samples (1.00%)\n",
      "Pneumonia: 27 positive samples (6.75%)\n",
      "Pneumothorax: 15 positive samples (3.75%)\n",
      "Support Devices: 90 positive samples (22.50%)\n",
      "Total samples in Validation (used): 400\n",
      "\n",
      "\n",
      "Label distribution in Testing (used) set:\n",
      "Atelectasis: 69 positive samples (17.25%)\n",
      "Cardiomegaly: 70 positive samples (17.50%)\n",
      "Consolidation: 15 positive samples (3.75%)\n",
      "Edema: 39 positive samples (9.75%)\n",
      "Enlarged Cardiomediastinum: 10 positive samples (2.50%)\n",
      "Fracture: 8 positive samples (2.00%)\n",
      "Lung Lesion: 11 positive samples (2.75%)\n",
      "Lung Opacity: 81 positive samples (20.25%)\n",
      "No Finding: 152 positive samples (38.00%)\n",
      "Pleural Effusion: 82 positive samples (20.50%)\n",
      "Pleural Other: 4 positive samples (1.00%)\n",
      "Pneumonia: 27 positive samples (6.75%)\n",
      "Pneumothorax: 15 positive samples (3.75%)\n",
      "Support Devices: 90 positive samples (22.50%)\n",
      "Total samples in Testing (used): 400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13joe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Creating datasets\n",
      "INFO:__main__:Creating dataloaders\n",
      "INFO:__main__:Initializing model\n",
      "C:\\Users\\13joe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Starting training\n",
      "INFO:__main__:Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3924757b95141fea8593bbe4e6b0b88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0236\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 01 | Loss: 3.0605 | Val F1: 0.0678 | BLEU-4: 0.4065 | ROUGE-L: 0.3036\n",
      "INFO:__main__:New best model saved\n",
      "INFO:__main__:Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd2adbbbf7dd4797ba9fd10f1f470080"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 02 | Loss: 2.9666 | Val F1: 0.0699 | BLEU-4: 0.3914 | ROUGE-L: 0.3093\n",
      "INFO:__main__:New best model saved\n",
      "INFO:__main__:Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2c3abaac8f4409da11dda61b98f6693"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 03 | Loss: 2.8890 | Val F1: 0.0778 | BLEU-4: 0.3914 | ROUGE-L: 0.2868\n",
      "INFO:__main__:New best model saved\n",
      "INFO:__main__:Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab05954505ec46d7b37390f8ee13e380"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 04 | Loss: 2.8181 | Val F1: 0.0736 | BLEU-4: 0.3938 | ROUGE-L: 0.2853\n",
      "INFO:__main__:Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf47e13656884a00842edb4d85c6496f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 05 | Loss: 2.7666 | Val F1: 0.0728 | BLEU-4: 0.3950 | ROUGE-L: 0.2919\n",
      "INFO:__main__:Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afee90cf3f38403887f7400a8d6b1934"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 06 | Loss: 2.7119 | Val F1: 0.0620 | BLEU-4: 0.4125 | ROUGE-L: 0.3017\n",
      "INFO:__main__:Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "442d6bc978654304ac5673e7a0554783"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 07 | Loss: 2.6576 | Val F1: 0.0531 | BLEU-4: 0.4233 | ROUGE-L: 0.3106\n",
      "INFO:__main__:Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "078711e3b57f45748f7d1c59bf351e4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 08 | Loss: 2.6040 | Val F1: 0.0492 | BLEU-4: 0.4304 | ROUGE-L: 0.3180\n",
      "INFO:__main__:Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06bd4fa261674058a995614c22f783e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 09 | Loss: 2.5506 | Val F1: 0.0320 | BLEU-4: 0.4280 | ROUGE-L: 0.3322\n",
      "INFO:__main__:Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de8a14be4cf24826a9ba376860bfe7e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:Epoch 10 | Loss: 2.5084 | Val F1: 0.0132 | BLEU-4: 0.4350 | ROUGE-L: 0.3462\n",
      "INFO:__main__:Training completed. Best validation Macro F1: 0.0778\n",
      "INFO:__main__:\n",
      "Testing with adjusted thresholds...\n",
      "INFO:__main__:\n",
      "Rare Class Performance:\n",
      "INFO:__main__:Pleural Other: 0.0000\n",
      "INFO:__main__:Fracture: 0.0000\n",
      "INFO:__main__:Enlarged Cardiomediastinum: 0.0000\n",
      "INFO:__main__:\n",
      "Generated Reports (First 5):\n",
      "INFO:__main__:\n",
      "Sample 1:\n",
      "INFO:__main__:Generated: . _ _ _ _ _ _ _ _ _. _ _ _ _ _ _ _. _ _ _ _. _ _. _ _ _. _ _ _ _ _ _ _ _ _ _ _. _ _ _ _ _ _ _ _ _ _ _ _ _ _. _ _ _ _ _ _ _ _ _ _ _ transportation. _ _... _ _ _ : _ _ _ _ _. _ _.. _.. _ _ _ _ प _.. _ _ _. _ _ _ _. _ :...... _... Siemens Siemens geological :. _. _ _. _. _.. : : _ _ : : : adaptations vegetable Reef. tributaries :bbed......... _rants : : _ histories : _ Transylvania _ _ _.. _ _. : _.. _ _... _ : : : eyebrow : :. _ histories adaptations _ encourage. Siemens _ _......... _ _ : Transylvaniaboro : _ splendid : Mr _ Beaver Divine Divine _ wrapped _ _ Human........ _... adaptations Ü. RTÉ... the vegetable RTÉ :. Siemens\n",
      "INFO:__main__:Reference: ['final report exam : chest frontal and lateral views. clinical information : mechanical fall from standing, on coumadin. neck pain. comparison : none. findings : frontal and lateral views of the chest were obtained. there is mild basilar atelectasis without evidence of focal consolidation. no pleural effusion or pneumothorax is seen. there is minimal biapical pleural thickening. cardiac silhouette is top normal with likely adjacent epicardial fat pad. the aorta is calcified an']\n",
      "INFO:__main__:--------------------------------------------------------------------------------\n",
      "INFO:__main__:\n",
      "Sample 2:\n",
      "INFO:__main__:Generated: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ : _ _ _ _ _ _ gazed _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ oxidation _ _ _ _ _ _ _ _ _ _ _ _ _ _ Delaware MTV _ _ _ _ _ _ _ _ _ _ rushed _. _ _. _ begging. _ _ _ _ _. _. flicker _ _. _ circled _ _ _ _ _. _ :... _ _ _ _ _alytic : _ :.alytic : contribution :.... _ _ _ _ _ _ _ _ _ _ _. _ curated contribution sts : RTÉ RTÉ. : RTÉ RTÉ.. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Humanoam _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _alytic contribution circled :. :. RTÉ... : : oxidation turning _ _ _ _ _ _ _ _ _ _ _ _alytic circled Finally : RTÉ RTÉ. Finally :.... Mr Human : Box.... : _\n",
      "INFO:__main__:Reference: ['final report examination : chest ( portable ap ) chest ( portable ap ) i indication : _ _ _ year old man with copious secretions, esophgeal ulcers. / / please evaluate for interval change comparison : chest radiographs _ _ _. impression : mild interstitial pulmonary edema unchanged since _ _ _. moderate cardiomegaly and pulmonary vascular congestion and left lower lobe atelectasis, probably due to the impact of the left ventricle, are all long - standing. left internal jugular line ends in']\n",
      "INFO:__main__:--------------------------------------------------------------------------------\n",
      "INFO:__main__:\n",
      "Sample 3:\n",
      "INFO:__main__:Generated: . _ _ _ _ _ _ _ _. _ _ _ _ _ _ _ _. _ _ _. _ _. _. _ _ _ _. _ _ _ _ _ _ _ _ _ _ _ transportation. _ _.. _ _... _.. _ _ _ _. _.. learns.... _ _ _ _ _.. _ _ _ _. :. _.. _ _.. _ _ _ _ _ _. _. _........... _ vegetable. : :........ _ Kumar :ppel Katrina.. _ _ wrapped Beaver psychic Divine. ग :..... _ _ _.. outreach... _.. _ loftalytic Box RTÉ _. _. _ _.. _ _..ppelppel. Mr. :. _ _ Transylvania Transylvania psychic Box Box. _ _...... educate... outreach... : :.. _ loft banged outreach.. geological : Box................. : :. : : Human Mission Mr : गdious :.......\n",
      "INFO:__main__:Reference: ['final report history : dyspnea. technique : frontal lateral views of the chest. comparison : none. findings : the lungs are clear without focal consolidation. no pleural effusion or pneumothorax is seen. the cardiac silhouette is top - normal. the italic contours are unremarkable. no pulmonary edema is seen. large air - fluid level is incidentally noted in the stomach impression : no acute cardiopulmonary process.']\n",
      "INFO:__main__:--------------------------------------------------------------------------------\n",
      "INFO:__main__:\n",
      "Sample 4:\n",
      "INFO:__main__:Generated: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "INFO:__main__:Reference: ['wet read : _ _ _ _ _ _ 11 : 24 pm interval extubation, with increased retrocardiac / left lower lobe opacity. the findings were discussed by dr. _ _ _ with dr. _ _ _ on the telephone on _ _ _ at 11 : 15 pm, 45 minutes after discovery of the findings. wet read version # 1 _ _ _ _ _ _ _ _ _ 11 : 21 pm interval extubation, with increased retrocardiac / left lower lobe opacity. the findings were discussed by dr. _ _ _ with dr. _ _ _ on the _ _ _ _ _ _ at 11 : 15 pm, 45 minutes after discovery of the findings. _ _ _']\n",
      "INFO:__main__:--------------------------------------------------------------------------------\n",
      "INFO:__main__:\n",
      "Sample 5:\n",
      "INFO:__main__:Generated: _ _ _ _ _ _ _ _ _ _ _ _ _ _rl bitesaway _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. _. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ps. _ _ _ cut _ blastedgies. _ _ _ _.. _ _ _ _... _ _ _ : _ _ _ _ _ _ _ _ _ : : Center :.. _ _ _ _ _ _ _ _ _ _ _ _ : _ : Mr : _ Siemens : Center : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Siemens Siemens Mr : : :.... _. freed Endowment : _ _ _ : _ _ _ _ _ : _ _ Mr : _ Mr _ _.. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. : papal _ _ _..... : : _ _ _ _ _ _ _ _ : _ _ : _ _ _. contribution Pierceoam contributionrga Center : transferring _\n",
      "INFO:__main__:Reference: ['final report indication : _ _ _ m with fever, immunosupressed / / eval for pna technique : pa and lateral views of the chest. comparison : _ _ _. findings : oblong opacity projecting over the right upper lung is compatible with calcified pleural plaque. the lungs are otherwise clear. no obvious effusion identified noting that there is exclusion of the right lateral costophrenic angle on the frontal view. the cardiomediastinal silhouette is stable given differences in projection.']\n",
      "INFO:__main__:--------------------------------------------------------------------------------\n",
      "INFO:__main__:\n",
      "=== Final Test Metrics ===\n",
      "INFO:__main__:Clinical Metrics:\n",
      "INFO:__main__:  Macro F1: 0.0744\n",
      "INFO:__main__:  Macro AUROC: 0.5040\n",
      "INFO:__main__:\n",
      "Report Generation Metrics:\n",
      "INFO:__main__:  BLEU-4: 0.3677\n",
      "INFO:__main__:  ROUGE-L: 0.2914\n",
      "INFO:__main__:\n",
      "Threshold-Adjusted Performance:\n",
      "INFO:__main__:  Macro F1: 0.0077\n",
      "INFO:__main__:  Macro AUROC: 0.4986\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
